{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sq_c2_s0.1_b16_lr0.001_d0.5_e10\n",
      "models/model_sq_c2_s0.1_b16_lr0.001_d0.5_e10.pth\n",
      "True\n",
      "1\n",
      "GeForce MX150\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Para:\n",
    "    # tensor_board_log_dir = 'runs/exp0'\n",
    "    feature_column_start_name = 'ep_ratio_ttm'\n",
    "    feature_column_end_name = 'BR'\n",
    "\n",
    "    # 模型设置\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    classification = 2 # 2, 3\n",
    "\n",
    "    # 权重\n",
    "    cross_weight = list()\n",
    "    if classification == 3:\n",
    "        cross_weight = [1.0, 1.0 ,1.0]\n",
    "    elif classification == 2:\n",
    "        cross_weight = [1.0, 1.0]\n",
    "    elif classification == 5:\n",
    "        cross_weight = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "    batch_size = 16\n",
    "    lr = 1e-3\n",
    "    drop = 0.5\n",
    "    epochs = 10\n",
    "\n",
    "    # 数据集设置\n",
    "    month_in_sample = range(0, 1)\n",
    "    # month_test = range(36, 48)\n",
    "\n",
    "    percent_cv = 0.1 # 10% cross validation\n",
    "\n",
    "    data_path = 'data/sq_space_1d_rate_20d_17-21_pre'\n",
    "\n",
    "\n",
    "    seed = 2022\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    info_str0 = data_path[5:7]+'_'+'c'+str(classification)+'_s'+str(percent_cv)\n",
    "    info_str1 = '_b'+str(batch_size)+'_lr'+str(lr)+'_d'+str(drop)+'_e'+str(epochs)\n",
    "    info_str = info_str0 + info_str1\n",
    "\n",
    "    save_model_path = 'models/'+'model_'+info_str+'.pth'\n",
    "\n",
    "para = Para()\n",
    "print(para.info_str)\n",
    "print(para.save_model_path)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      return_bin order_book_id board_type            sector_code  month  \\\n0            0.0   600104.XSHG  MainBoard  ConsumerDiscretionary      0   \n1            0.0   600104.XSHG  MainBoard  ConsumerDiscretionary      1   \n2            0.0   600104.XSHG  MainBoard  ConsumerDiscretionary      2   \n3            0.0   600104.XSHG  MainBoard  ConsumerDiscretionary      3   \n4            0.0   600104.XSHG  MainBoard  ConsumerDiscretionary      4   \n...          ...           ...        ...                    ...    ...   \n1189         0.0   600104.XSHG  MainBoard  ConsumerDiscretionary   1192   \n1190         0.0   600104.XSHG  MainBoard  ConsumerDiscretionary   1193   \n1191         0.0   600104.XSHG  MainBoard  ConsumerDiscretionary   1194   \n1192         1.0   600104.XSHG  MainBoard  ConsumerDiscretionary   1195   \n1193         0.0   600104.XSHG  MainBoard  ConsumerDiscretionary   1196   \n\n            date  yield_rate  ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  ...  \\\n0     2017-01-03    0.809567      1.284306      0.358685     -0.160216  ...   \n1     2017-01-04    0.428012      1.152139      0.425961     -0.255248  ...   \n2     2017-01-05    0.509194      1.230912      0.385595     -0.198608  ...   \n3     2017-01-06    0.638719      1.277592      0.362049     -0.165043  ...   \n4     2017-01-09    0.316583      1.194613      0.404096     -0.224708  ...   \n...          ...         ...           ...           ...           ...  ...   \n1189  2021-11-29    0.417490      0.131692     -1.265140      1.100538  ...   \n1190  2021-11-30    0.532614      0.179925     -1.281825      1.149883  ...   \n1191  2021-12-01    0.207170      0.097650     -1.253223      1.065712  ...   \n1192  2021-12-02   -0.091058     -0.035215     -1.205553      0.929787  ...   \n1193  2021-12-03    0.017530     -0.012337     -1.213895      0.953191  ...   \n\n         RSI10        SY    BIAS20     VOL30     VOL60    VOL120    VOLT20  \\\n0     0.988857  0.500157  0.253561  1.023958  0.067001 -0.342131 -0.402515   \n1     1.218035  0.500157  0.657302  1.056231  0.093217 -0.333081 -0.428033   \n2     0.870098 -0.250079  0.441853  0.991284  0.105760 -0.330882 -0.477653   \n3     0.407437 -0.250079  0.348336  0.794401  0.121854 -0.347766 -0.610628   \n4     0.954654  0.500157  0.626933  0.716922  0.148316 -0.346567 -0.731410   \n...        ...       ...       ...       ...       ...       ...       ...   \n1189 -1.323734 -0.250079 -1.043171  1.331187  1.138590  1.174348  0.272634   \n1190 -1.036148 -1.000314 -1.183964  1.311968  1.097471  1.174860  0.317672   \n1191 -0.840902 -0.250079 -0.918093  1.276317  1.038991  1.174089  0.317672   \n1192 -0.097430 -0.250079 -0.471171  1.291874  1.040739  1.191806  0.322346   \n1193 -0.229598 -1.000314 -0.428440  1.309124  1.015012  1.205788  0.033962   \n\n        VOLT60        AR        BR  \n0    -0.940741 -0.185952 -0.682637  \n1    -0.957848  0.489018 -0.155550  \n2    -0.993590 -0.566329 -0.978739  \n3    -1.023715 -0.485009 -0.883754  \n4    -1.051159 -0.492318 -1.017275  \n...        ...       ...       ...  \n1189 -0.653109 -0.360370 -0.445948  \n1190 -0.699096 -0.414854 -0.318488  \n1191 -0.722895 -0.199559 -0.229953  \n1192 -0.752404 -0.000075  0.027994  \n1193 -0.768220  0.005327  0.089914  \n\n[1194 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>return_bin</th>\n      <th>order_book_id</th>\n      <th>board_type</th>\n      <th>sector_code</th>\n      <th>month</th>\n      <th>date</th>\n      <th>yield_rate</th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>...</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>0</td>\n      <td>2017-01-03</td>\n      <td>0.809567</td>\n      <td>1.284306</td>\n      <td>0.358685</td>\n      <td>-0.160216</td>\n      <td>...</td>\n      <td>0.988857</td>\n      <td>0.500157</td>\n      <td>0.253561</td>\n      <td>1.023958</td>\n      <td>0.067001</td>\n      <td>-0.342131</td>\n      <td>-0.402515</td>\n      <td>-0.940741</td>\n      <td>-0.185952</td>\n      <td>-0.682637</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>1</td>\n      <td>2017-01-04</td>\n      <td>0.428012</td>\n      <td>1.152139</td>\n      <td>0.425961</td>\n      <td>-0.255248</td>\n      <td>...</td>\n      <td>1.218035</td>\n      <td>0.500157</td>\n      <td>0.657302</td>\n      <td>1.056231</td>\n      <td>0.093217</td>\n      <td>-0.333081</td>\n      <td>-0.428033</td>\n      <td>-0.957848</td>\n      <td>0.489018</td>\n      <td>-0.155550</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>2</td>\n      <td>2017-01-05</td>\n      <td>0.509194</td>\n      <td>1.230912</td>\n      <td>0.385595</td>\n      <td>-0.198608</td>\n      <td>...</td>\n      <td>0.870098</td>\n      <td>-0.250079</td>\n      <td>0.441853</td>\n      <td>0.991284</td>\n      <td>0.105760</td>\n      <td>-0.330882</td>\n      <td>-0.477653</td>\n      <td>-0.993590</td>\n      <td>-0.566329</td>\n      <td>-0.978739</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>3</td>\n      <td>2017-01-06</td>\n      <td>0.638719</td>\n      <td>1.277592</td>\n      <td>0.362049</td>\n      <td>-0.165043</td>\n      <td>...</td>\n      <td>0.407437</td>\n      <td>-0.250079</td>\n      <td>0.348336</td>\n      <td>0.794401</td>\n      <td>0.121854</td>\n      <td>-0.347766</td>\n      <td>-0.610628</td>\n      <td>-1.023715</td>\n      <td>-0.485009</td>\n      <td>-0.883754</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>4</td>\n      <td>2017-01-09</td>\n      <td>0.316583</td>\n      <td>1.194613</td>\n      <td>0.404096</td>\n      <td>-0.224708</td>\n      <td>...</td>\n      <td>0.954654</td>\n      <td>0.500157</td>\n      <td>0.626933</td>\n      <td>0.716922</td>\n      <td>0.148316</td>\n      <td>-0.346567</td>\n      <td>-0.731410</td>\n      <td>-1.051159</td>\n      <td>-0.492318</td>\n      <td>-1.017275</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1189</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>1192</td>\n      <td>2021-11-29</td>\n      <td>0.417490</td>\n      <td>0.131692</td>\n      <td>-1.265140</td>\n      <td>1.100538</td>\n      <td>...</td>\n      <td>-1.323734</td>\n      <td>-0.250079</td>\n      <td>-1.043171</td>\n      <td>1.331187</td>\n      <td>1.138590</td>\n      <td>1.174348</td>\n      <td>0.272634</td>\n      <td>-0.653109</td>\n      <td>-0.360370</td>\n      <td>-0.445948</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>1193</td>\n      <td>2021-11-30</td>\n      <td>0.532614</td>\n      <td>0.179925</td>\n      <td>-1.281825</td>\n      <td>1.149883</td>\n      <td>...</td>\n      <td>-1.036148</td>\n      <td>-1.000314</td>\n      <td>-1.183964</td>\n      <td>1.311968</td>\n      <td>1.097471</td>\n      <td>1.174860</td>\n      <td>0.317672</td>\n      <td>-0.699096</td>\n      <td>-0.414854</td>\n      <td>-0.318488</td>\n    </tr>\n    <tr>\n      <th>1191</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>1194</td>\n      <td>2021-12-01</td>\n      <td>0.207170</td>\n      <td>0.097650</td>\n      <td>-1.253223</td>\n      <td>1.065712</td>\n      <td>...</td>\n      <td>-0.840902</td>\n      <td>-0.250079</td>\n      <td>-0.918093</td>\n      <td>1.276317</td>\n      <td>1.038991</td>\n      <td>1.174089</td>\n      <td>0.317672</td>\n      <td>-0.722895</td>\n      <td>-0.199559</td>\n      <td>-0.229953</td>\n    </tr>\n    <tr>\n      <th>1192</th>\n      <td>1.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>1195</td>\n      <td>2021-12-02</td>\n      <td>-0.091058</td>\n      <td>-0.035215</td>\n      <td>-1.205553</td>\n      <td>0.929787</td>\n      <td>...</td>\n      <td>-0.097430</td>\n      <td>-0.250079</td>\n      <td>-0.471171</td>\n      <td>1.291874</td>\n      <td>1.040739</td>\n      <td>1.191806</td>\n      <td>0.322346</td>\n      <td>-0.752404</td>\n      <td>-0.000075</td>\n      <td>0.027994</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>0.0</td>\n      <td>600104.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerDiscretionary</td>\n      <td>1196</td>\n      <td>2021-12-03</td>\n      <td>0.017530</td>\n      <td>-0.012337</td>\n      <td>-1.213895</td>\n      <td>0.953191</td>\n      <td>...</td>\n      <td>-0.229598</td>\n      <td>-1.000314</td>\n      <td>-0.428440</td>\n      <td>1.309124</td>\n      <td>1.015012</td>\n      <td>1.205788</td>\n      <td>0.033962</td>\n      <td>-0.768220</td>\n      <td>0.005327</td>\n      <td>0.089914</td>\n    </tr>\n  </tbody>\n</table>\n<p>1194 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_in_sample = None\n",
    "for i_month in para.month_in_sample:\n",
    "    file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "    data_curr_month = pd.read_csv(file_name)\n",
    "\n",
    "    data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "    data_curr_month.insert(loc=0, column='return_bin', value=np.nan)\n",
    "\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']>0, 'return_bin'] = 0\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']<=0, 'return_bin'] = 1\n",
    "\n",
    "    if i_month == para.month_in_sample[0]:\n",
    "        data_in_sample = data_curr_month\n",
    "    else:\n",
    "        data_in_sample = pd.concat([data_in_sample, data_curr_month])\n",
    "        # data_in_sample = data_in_sample.append(data_curr_month)\n",
    "\n",
    "data_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0         1.284306      0.358685     -0.160216  -0.431525 -0.477758   \n1         1.152139      0.425961     -0.255248  -0.249418 -0.436122   \n2         1.230912      0.385595     -0.198608  -0.145385 -0.380331   \n3         1.277592      0.362049     -0.165043  -0.086603 -0.322995   \n4         1.194613      0.404096     -0.224708   0.002761 -0.257813   \n...            ...           ...           ...        ...       ...   \n1069      0.594835     -1.120446      1.256091   0.037133  0.018016   \n1070      0.656485     -1.141604      1.316486   0.017431  0.018166   \n1071      0.693237     -1.154049      1.352490  -0.015745  0.011115   \n1072      0.719180     -1.162761      1.377906  -0.053966 -0.002785   \n1073      0.696932     -1.155294      1.356110  -0.073471 -0.018120   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0      0.030239  0.988857  0.500157  0.253561  1.023958  0.067001 -0.342131   \n1      0.451899  1.218035  0.500157  0.657302  1.056231  0.093217 -0.333081   \n2      0.605792  0.870098 -0.250079  0.441853  0.991284  0.105760 -0.330882   \n3      0.622588  0.407437 -0.250079  0.348336  0.794401  0.121854 -0.347766   \n4      0.707880  0.954654  0.500157  0.626933  0.716922  0.148316 -0.346567   \n...         ...       ...       ...       ...       ...       ...       ...   \n1069   0.059125 -0.558520  0.500157  0.079456 -0.134044 -0.373492  1.230104   \n1070   0.000856 -0.461329  0.500157 -0.112962 -0.236794 -0.378449  1.223895   \n1071  -0.077414 -0.173590  0.500157 -0.238283 -0.292676 -0.390751  1.209950   \n1072  -0.151887  0.172847 -0.250079 -0.305984 -0.326746 -0.417233  1.176037   \n1073  -0.167489 -0.390835 -0.250079 -0.238390 -0.327318 -0.436272  1.156062   \n\n        VOLT20    VOLT60        AR        BR  \n0    -0.402515 -0.940741 -0.185952 -0.682637  \n1    -0.428033 -0.957848  0.489018 -0.155550  \n2    -0.477653 -0.993590 -0.566329 -0.978739  \n3    -0.610628 -1.023715 -0.485009 -0.883754  \n4    -0.731410 -1.051159 -0.492318 -1.017275  \n...        ...       ...       ...       ...  \n1069 -1.477572 -1.585713  0.709617  0.168198  \n1070 -1.475688 -1.608194  0.734207  0.217079  \n1071 -1.518329 -1.612158  0.842647  0.257739  \n1072 -1.492499 -1.616117  0.894925  0.327854  \n1073 -1.495674 -1.617859  1.222292  0.521342  \n\n[1074 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.284306</td>\n      <td>0.358685</td>\n      <td>-0.160216</td>\n      <td>-0.431525</td>\n      <td>-0.477758</td>\n      <td>0.030239</td>\n      <td>0.988857</td>\n      <td>0.500157</td>\n      <td>0.253561</td>\n      <td>1.023958</td>\n      <td>0.067001</td>\n      <td>-0.342131</td>\n      <td>-0.402515</td>\n      <td>-0.940741</td>\n      <td>-0.185952</td>\n      <td>-0.682637</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.152139</td>\n      <td>0.425961</td>\n      <td>-0.255248</td>\n      <td>-0.249418</td>\n      <td>-0.436122</td>\n      <td>0.451899</td>\n      <td>1.218035</td>\n      <td>0.500157</td>\n      <td>0.657302</td>\n      <td>1.056231</td>\n      <td>0.093217</td>\n      <td>-0.333081</td>\n      <td>-0.428033</td>\n      <td>-0.957848</td>\n      <td>0.489018</td>\n      <td>-0.155550</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.230912</td>\n      <td>0.385595</td>\n      <td>-0.198608</td>\n      <td>-0.145385</td>\n      <td>-0.380331</td>\n      <td>0.605792</td>\n      <td>0.870098</td>\n      <td>-0.250079</td>\n      <td>0.441853</td>\n      <td>0.991284</td>\n      <td>0.105760</td>\n      <td>-0.330882</td>\n      <td>-0.477653</td>\n      <td>-0.993590</td>\n      <td>-0.566329</td>\n      <td>-0.978739</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.277592</td>\n      <td>0.362049</td>\n      <td>-0.165043</td>\n      <td>-0.086603</td>\n      <td>-0.322995</td>\n      <td>0.622588</td>\n      <td>0.407437</td>\n      <td>-0.250079</td>\n      <td>0.348336</td>\n      <td>0.794401</td>\n      <td>0.121854</td>\n      <td>-0.347766</td>\n      <td>-0.610628</td>\n      <td>-1.023715</td>\n      <td>-0.485009</td>\n      <td>-0.883754</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.194613</td>\n      <td>0.404096</td>\n      <td>-0.224708</td>\n      <td>0.002761</td>\n      <td>-0.257813</td>\n      <td>0.707880</td>\n      <td>0.954654</td>\n      <td>0.500157</td>\n      <td>0.626933</td>\n      <td>0.716922</td>\n      <td>0.148316</td>\n      <td>-0.346567</td>\n      <td>-0.731410</td>\n      <td>-1.051159</td>\n      <td>-0.492318</td>\n      <td>-1.017275</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.594835</td>\n      <td>-1.120446</td>\n      <td>1.256091</td>\n      <td>0.037133</td>\n      <td>0.018016</td>\n      <td>0.059125</td>\n      <td>-0.558520</td>\n      <td>0.500157</td>\n      <td>0.079456</td>\n      <td>-0.134044</td>\n      <td>-0.373492</td>\n      <td>1.230104</td>\n      <td>-1.477572</td>\n      <td>-1.585713</td>\n      <td>0.709617</td>\n      <td>0.168198</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.656485</td>\n      <td>-1.141604</td>\n      <td>1.316486</td>\n      <td>0.017431</td>\n      <td>0.018166</td>\n      <td>0.000856</td>\n      <td>-0.461329</td>\n      <td>0.500157</td>\n      <td>-0.112962</td>\n      <td>-0.236794</td>\n      <td>-0.378449</td>\n      <td>1.223895</td>\n      <td>-1.475688</td>\n      <td>-1.608194</td>\n      <td>0.734207</td>\n      <td>0.217079</td>\n    </tr>\n    <tr>\n      <th>1071</th>\n      <td>0.693237</td>\n      <td>-1.154049</td>\n      <td>1.352490</td>\n      <td>-0.015745</td>\n      <td>0.011115</td>\n      <td>-0.077414</td>\n      <td>-0.173590</td>\n      <td>0.500157</td>\n      <td>-0.238283</td>\n      <td>-0.292676</td>\n      <td>-0.390751</td>\n      <td>1.209950</td>\n      <td>-1.518329</td>\n      <td>-1.612158</td>\n      <td>0.842647</td>\n      <td>0.257739</td>\n    </tr>\n    <tr>\n      <th>1072</th>\n      <td>0.719180</td>\n      <td>-1.162761</td>\n      <td>1.377906</td>\n      <td>-0.053966</td>\n      <td>-0.002785</td>\n      <td>-0.151887</td>\n      <td>0.172847</td>\n      <td>-0.250079</td>\n      <td>-0.305984</td>\n      <td>-0.326746</td>\n      <td>-0.417233</td>\n      <td>1.176037</td>\n      <td>-1.492499</td>\n      <td>-1.616117</td>\n      <td>0.894925</td>\n      <td>0.327854</td>\n    </tr>\n    <tr>\n      <th>1073</th>\n      <td>0.696932</td>\n      <td>-1.155294</td>\n      <td>1.356110</td>\n      <td>-0.073471</td>\n      <td>-0.018120</td>\n      <td>-0.167489</td>\n      <td>-0.390835</td>\n      <td>-0.250079</td>\n      <td>-0.238390</td>\n      <td>-0.327318</td>\n      <td>-0.436272</td>\n      <td>1.156062</td>\n      <td>-1.495674</td>\n      <td>-1.617859</td>\n      <td>1.222292</td>\n      <td>0.521342</td>\n    </tr>\n  </tbody>\n</table>\n<p>1074 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in_sample = data_in_sample.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "y_in_sample = data_in_sample.loc[:, 'return_bin']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False) # True, random_state=para.seed)\n",
    "# X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n1069    0.0\n1070    0.0\n1071    0.0\n1072    0.0\n1073    0.0\nName: return_bin, Length: 1074, dtype: float64"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0         1.284306      0.358685     -0.160216  -0.431525 -0.477758   \n1         1.152139      0.425961     -0.255248  -0.249418 -0.436122   \n2         1.230912      0.385595     -0.198608  -0.145385 -0.380331   \n3         1.277592      0.362049     -0.165043  -0.086603 -0.322995   \n4         1.194613      0.404096     -0.224708   0.002761 -0.257813   \n...            ...           ...           ...        ...       ...   \n1069      0.594835     -1.120446      1.256091   0.037133  0.018016   \n1070      0.656485     -1.141604      1.316486   0.017431  0.018166   \n1071      0.693237     -1.154049      1.352490  -0.015745  0.011115   \n1072      0.719180     -1.162761      1.377906  -0.053966 -0.002785   \n1073      0.696932     -1.155294      1.356110  -0.073471 -0.018120   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0      0.030239  0.988857  0.500157  0.253561  1.023958  0.067001 -0.342131   \n1      0.451899  1.218035  0.500157  0.657302  1.056231  0.093217 -0.333081   \n2      0.605792  0.870098 -0.250079  0.441853  0.991284  0.105760 -0.330882   \n3      0.622588  0.407437 -0.250079  0.348336  0.794401  0.121854 -0.347766   \n4      0.707880  0.954654  0.500157  0.626933  0.716922  0.148316 -0.346567   \n...         ...       ...       ...       ...       ...       ...       ...   \n1069   0.059125 -0.558520  0.500157  0.079456 -0.134044 -0.373492  1.230104   \n1070   0.000856 -0.461329  0.500157 -0.112962 -0.236794 -0.378449  1.223895   \n1071  -0.077414 -0.173590  0.500157 -0.238283 -0.292676 -0.390751  1.209950   \n1072  -0.151887  0.172847 -0.250079 -0.305984 -0.326746 -0.417233  1.176037   \n1073  -0.167489 -0.390835 -0.250079 -0.238390 -0.327318 -0.436272  1.156062   \n\n        VOLT20    VOLT60        AR        BR  return_bin  \n0    -0.402515 -0.940741 -0.185952 -0.682637         0.0  \n1    -0.428033 -0.957848  0.489018 -0.155550         0.0  \n2    -0.477653 -0.993590 -0.566329 -0.978739         0.0  \n3    -0.610628 -1.023715 -0.485009 -0.883754         0.0  \n4    -0.731410 -1.051159 -0.492318 -1.017275         0.0  \n...        ...       ...       ...       ...         ...  \n1069 -1.477572 -1.585713  0.709617  0.168198         0.0  \n1070 -1.475688 -1.608194  0.734207  0.217079         0.0  \n1071 -1.518329 -1.612158  0.842647  0.257739         0.0  \n1072 -1.492499 -1.616117  0.894925  0.327854         0.0  \n1073 -1.495674 -1.617859  1.222292  0.521342         0.0  \n\n[1074 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n      <th>return_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.284306</td>\n      <td>0.358685</td>\n      <td>-0.160216</td>\n      <td>-0.431525</td>\n      <td>-0.477758</td>\n      <td>0.030239</td>\n      <td>0.988857</td>\n      <td>0.500157</td>\n      <td>0.253561</td>\n      <td>1.023958</td>\n      <td>0.067001</td>\n      <td>-0.342131</td>\n      <td>-0.402515</td>\n      <td>-0.940741</td>\n      <td>-0.185952</td>\n      <td>-0.682637</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.152139</td>\n      <td>0.425961</td>\n      <td>-0.255248</td>\n      <td>-0.249418</td>\n      <td>-0.436122</td>\n      <td>0.451899</td>\n      <td>1.218035</td>\n      <td>0.500157</td>\n      <td>0.657302</td>\n      <td>1.056231</td>\n      <td>0.093217</td>\n      <td>-0.333081</td>\n      <td>-0.428033</td>\n      <td>-0.957848</td>\n      <td>0.489018</td>\n      <td>-0.155550</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.230912</td>\n      <td>0.385595</td>\n      <td>-0.198608</td>\n      <td>-0.145385</td>\n      <td>-0.380331</td>\n      <td>0.605792</td>\n      <td>0.870098</td>\n      <td>-0.250079</td>\n      <td>0.441853</td>\n      <td>0.991284</td>\n      <td>0.105760</td>\n      <td>-0.330882</td>\n      <td>-0.477653</td>\n      <td>-0.993590</td>\n      <td>-0.566329</td>\n      <td>-0.978739</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.277592</td>\n      <td>0.362049</td>\n      <td>-0.165043</td>\n      <td>-0.086603</td>\n      <td>-0.322995</td>\n      <td>0.622588</td>\n      <td>0.407437</td>\n      <td>-0.250079</td>\n      <td>0.348336</td>\n      <td>0.794401</td>\n      <td>0.121854</td>\n      <td>-0.347766</td>\n      <td>-0.610628</td>\n      <td>-1.023715</td>\n      <td>-0.485009</td>\n      <td>-0.883754</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.194613</td>\n      <td>0.404096</td>\n      <td>-0.224708</td>\n      <td>0.002761</td>\n      <td>-0.257813</td>\n      <td>0.707880</td>\n      <td>0.954654</td>\n      <td>0.500157</td>\n      <td>0.626933</td>\n      <td>0.716922</td>\n      <td>0.148316</td>\n      <td>-0.346567</td>\n      <td>-0.731410</td>\n      <td>-1.051159</td>\n      <td>-0.492318</td>\n      <td>-1.017275</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.594835</td>\n      <td>-1.120446</td>\n      <td>1.256091</td>\n      <td>0.037133</td>\n      <td>0.018016</td>\n      <td>0.059125</td>\n      <td>-0.558520</td>\n      <td>0.500157</td>\n      <td>0.079456</td>\n      <td>-0.134044</td>\n      <td>-0.373492</td>\n      <td>1.230104</td>\n      <td>-1.477572</td>\n      <td>-1.585713</td>\n      <td>0.709617</td>\n      <td>0.168198</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.656485</td>\n      <td>-1.141604</td>\n      <td>1.316486</td>\n      <td>0.017431</td>\n      <td>0.018166</td>\n      <td>0.000856</td>\n      <td>-0.461329</td>\n      <td>0.500157</td>\n      <td>-0.112962</td>\n      <td>-0.236794</td>\n      <td>-0.378449</td>\n      <td>1.223895</td>\n      <td>-1.475688</td>\n      <td>-1.608194</td>\n      <td>0.734207</td>\n      <td>0.217079</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1071</th>\n      <td>0.693237</td>\n      <td>-1.154049</td>\n      <td>1.352490</td>\n      <td>-0.015745</td>\n      <td>0.011115</td>\n      <td>-0.077414</td>\n      <td>-0.173590</td>\n      <td>0.500157</td>\n      <td>-0.238283</td>\n      <td>-0.292676</td>\n      <td>-0.390751</td>\n      <td>1.209950</td>\n      <td>-1.518329</td>\n      <td>-1.612158</td>\n      <td>0.842647</td>\n      <td>0.257739</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1072</th>\n      <td>0.719180</td>\n      <td>-1.162761</td>\n      <td>1.377906</td>\n      <td>-0.053966</td>\n      <td>-0.002785</td>\n      <td>-0.151887</td>\n      <td>0.172847</td>\n      <td>-0.250079</td>\n      <td>-0.305984</td>\n      <td>-0.326746</td>\n      <td>-0.417233</td>\n      <td>1.176037</td>\n      <td>-1.492499</td>\n      <td>-1.616117</td>\n      <td>0.894925</td>\n      <td>0.327854</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1073</th>\n      <td>0.696932</td>\n      <td>-1.155294</td>\n      <td>1.356110</td>\n      <td>-0.073471</td>\n      <td>-0.018120</td>\n      <td>-0.167489</td>\n      <td>-0.390835</td>\n      <td>-0.250079</td>\n      <td>-0.238390</td>\n      <td>-0.327318</td>\n      <td>-0.436272</td>\n      <td>1.156062</td>\n      <td>-1.495674</td>\n      <td>-1.617859</td>\n      <td>1.222292</td>\n      <td>0.521342</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1074 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_cv = pd.concat([X_cv, y_cv], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "X_train_ndarray = X_train.values\n",
    "y_train_ndarray = y_train.values\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_ndarray).type(torch.FloatTensor), torch.from_numpy(y_train_ndarray).type(torch.LongTensor))\n",
    "\n",
    "# for X_train_temp, y_train_temp in train_dataset:\n",
    "#     print(X_train_temp, y_train_temp)\n",
    "#     print(X_train_temp.dtype, y_train_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_cv_ndarray = X_cv.values\n",
    "y_cv_ndarray = y_cv.values\n",
    "\n",
    "cv_dataset = TensorDataset(torch.from_numpy(X_cv_ndarray).type(torch.FloatTensor), torch.from_numpy(y_cv.values).type(torch.LongTensor))\n",
    "\n",
    "# for X_cv_temp, y_cv_temp in cv_dataset:\n",
    "#     print(X_cv_temp, y_cv_temp)\n",
    "#     print(X_cv_temp.dtype, y_cv_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cv_dataloader = DataLoader(\n",
    "    dataset=cv_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_test = None\n",
    "# for i_month in para.month_test:\n",
    "#\n",
    "#     file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "#     data_curr_month = pd.read_csv(file_name)\n",
    "#\n",
    "#     data_curr_month = data_curr_month.dropna(axis=0)\n",
    "#\n",
    "#     data_curr_month = label_data(data=data_curr_month, percent_select=para.percent_select)\n",
    "#\n",
    "#     if i_month == para.month_test[0]:\n",
    "#         data_test = data_curr_month\n",
    "#     else:\n",
    "#         data_test = pd.concat([data_test, data_curr_month])\n",
    "#         # data_test = data_test.append(data_curr_month)\n",
    "#\n",
    "# X_test = data_test.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "# y_test = data_test.loc[:, 'return_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# X_test_ndarray = X_test.values\n",
    "# y_test_ndarray = y_test.values\n",
    "#\n",
    "# test_dataset = TensorDataset(torch.from_numpy(X_test_ndarray).type(torch.FloatTensor), torch.from_numpy(y_test_ndarray).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "#\n",
    "# test_dataloader = DataLoader(\n",
    "#     dataset=test_dataset,\n",
    "#     batch_size=para.batch_size,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from my_utils.model_class import MLP\n",
    "\n",
    "model = MLP(in_nums=len(X_train.columns), out_nums=para.classification, drop_p=para.drop)\n",
    "# to device\n",
    "model = model.to(device=para.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    train_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # to device\n",
    "        X = X.to(device=para.device)\n",
    "        y = y.to(device=para.device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # metric on current batch\n",
    "        train_precision(pred.argmax(1), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = train_precision.compute()\n",
    "    print(\"Precision of every train dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, train_loss, total_precision\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    test_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # to device\n",
    "            X = X.to(device=para.device)\n",
    "            y = y.to(device=para.device)\n",
    "\n",
    "            # compute prediction and loss\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # metric on current batch\n",
    "            test_precision(pred.argmax(1), y)\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = test_precision.compute()\n",
    "    print(\"Precision of every test dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, test_loss, total_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_df_to_dataloader(df: pd.DataFrame, select: int) -> DataLoader:\n",
    "\n",
    "    df = df[df['return_bin'] == select]\n",
    "\n",
    "    df_dataset = TensorDataset(\n",
    "        torch.from_numpy(df.loc[:, para.feature_column_start_name: para.feature_column_end_name].values).type(torch.FloatTensor),\n",
    "        torch.from_numpy(df.loc[:, 'return_bin'].values).type(torch.LongTensor))\n",
    "\n",
    "    df_dataloader = DataLoader(\n",
    "        dataset=df_dataset,\n",
    "        batch_size=para.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return df_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp2_dataloader = select_df_to_dataloader(df=data_cv, select=2)\n",
    "temp1_dataloader = select_df_to_dataloader(df=data_cv, select=1)\n",
    "temp0_dataloader = select_df_to_dataloader(df=data_cv, select=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 50.9%, Avg loss: 0.695894 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5012, 0.5145], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 47.5%, Avg loss: 0.689949 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5116, 0.3824], device='cuda:0')\n",
      "\n",
      "Time cost = 0.922371s\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 52.3%, Avg loss: 0.689486 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5207, 0.5247], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 52.5%, Avg loss: 0.690198 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5444, 0.4667], device='cuda:0')\n",
      "\n",
      "Time cost = 1.560776s\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.5%, Avg loss: 0.680858 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5981, 0.5518], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 52.5%, Avg loss: 0.689209 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5408, 0.4545], device='cuda:0')\n",
      "\n",
      "Time cost = 2.157658s\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.9%, Avg loss: 0.674158 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5931, 0.5572], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 56.7%, Avg loss: 0.687239 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5596, 0.6364], device='cuda:0')\n",
      "\n",
      "Time cost = 2.808466s\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.8%, Avg loss: 0.669609 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5912, 0.5701], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 56.7%, Avg loss: 0.682082 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5644, 0.5789], device='cuda:0')\n",
      "\n",
      "Time cost = 3.477310s\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.1%, Avg loss: 0.683793 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5819, 0.5639], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.677669 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5758, 0.6190], device='cuda:0')\n",
      "\n",
      "Time cost = 4.032157s\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.5%, Avg loss: 0.661216 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6220, 0.5801], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 50.8%, Avg loss: 0.678347 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5341, 0.4375], device='cuda:0')\n",
      "\n",
      "Time cost = 4.645518s\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.4%, Avg loss: 0.665522 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6111, 0.5833], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 52.5%, Avg loss: 0.670241 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5435, 0.4643], device='cuda:0')\n",
      "\n",
      "Time cost = 5.301646s\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.6%, Avg loss: 0.639740 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6370, 0.6173], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 53.3%, Avg loss: 0.672018 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5495, 0.4828], device='cuda:0')\n",
      "\n",
      "Time cost = 5.918909s\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.0%, Avg loss: 0.654033 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6167, 0.6046], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 55.8%, Avg loss: 0.670647 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5638, 0.5385], device='cuda:0')\n",
      "\n",
      "Time cost = 6.516531s\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 计时\n",
    "time_start = time.time()\n",
    "\n",
    "# writer = SummaryWriter(para.tensor_board_log_dir)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# to device\n",
    "loss_fn = loss_fn.to(device=para.device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=para.lr)\n",
    "\n",
    "epochs = para.epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    model.train()\n",
    "    accuracy_train, loss_train, precision_train = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    model.eval()\n",
    "    accuracy_cv, loss_cv, precision_cv = test_loop(cv_dataloader, model, loss_fn)\n",
    "\n",
    "    # accuracy2 = test_loop(temp2_dataloader, model, loss_fn)\n",
    "    # print('#')\n",
    "    # accuracy1 = test_loop(temp1_dataloader, model, loss_fn)\n",
    "    # accuracy0 = test_loop(temp0_dataloader, model, loss_fn)\n",
    "\n",
    "    # 写入 tensorboard\n",
    "    if para.classification == 2:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 3:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 5:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2],\n",
    "                               'precision3': precision_cv[3],\n",
    "                               'precision4': precision_cv[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2],\n",
    "                               'precision3': precision_train[3],\n",
    "                               'precision4': precision_train[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/cv',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_cv},\n",
    "                       global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/train',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_train},\n",
    "                       global_step=t)\n",
    "    writer.flush()\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Time cost = %fs' % (time_end - time_start))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 保存模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish save model!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), para.save_model_path)\n",
    "\n",
    "print('Finish save model!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # captum\n",
    "# from captum.attr import IntegratedGradients\n",
    "#\n",
    "# ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp = cv_dataloader.dataset.tensors[0]\n",
    "# temp.requires_grad_()\n",
    "# attr, delta = ig.attribute(temp,target=1, return_convergence_delta=True)\n",
    "# attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Helper method to print importances and visualize distribution\n",
    "# def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "#     print(title)\n",
    "#     for i in range(len(feature_names)):\n",
    "#         print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "#     y_pos = (np.arange(len(feature_names)))\n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(20,6))\n",
    "#         plt.barh(y_pos, importances, align='center')\n",
    "#         plt.yticks(y_pos, feature_names)\n",
    "#         plt.ylabel(axis_title)\n",
    "#         plt.grid(axis='y')\n",
    "#         plt.title(title)\n",
    "# visualize_importances(feature_names=X_cv.columns.values.tolist(), importances=np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_cv.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.Tensor(\n",
    "#     [[-0.0441,  0.0773],\n",
    "#     [-0.0781, -0.1772],\n",
    "#     [-0.1319, -0.0432],\n",
    "#     [-0.0714, -0.1261],\n",
    "#     [-0.0806, -0.1370],\n",
    "#     [-0.1730, -0.1472],\n",
    "#     [-0.0350, -0.0507],\n",
    "#     [-0.1149, -0.2248]])\n",
    "# # input = input.reshape(-1,4)\n",
    "# target = torch.Tensor([0, 1, 1, 0, 0, 0, 0, 0]).type(torch.LongTensor)\n",
    "# print(input.dtype)\n",
    "# print(target.dtype)\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.Tensor([1,4,1]).type(torch.LongTensor)\n",
    "# output = loss(input, target)\n",
    "# print(input,target,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# input = torch.Tensor([0.5, 0.4, 0.3])\n",
    "# target = torch.Tensor([0])\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}